---
title: "DM_project"
author: "Bai Xue"
date: "12/8/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Librarys
```{r}
library(ggplot2)
library(plyr)
library(ISLR)
library(MASS)
library(knitr)
library(gam)
library(readr)
library(glmnet)
library(rmarkdown)
library(rpart)
library(klaR) 
library(leaps) 
library(partykit)
library(rpart.plot)
install.packages()
```

## Read in data and read convert values to 0/1/NA
```{r, warning=FALSE, message = FALSE}
#linear regression
elementary <- read_csv("elementary.csv")

middle <- read_csv("middle.csv")

high <- read_csv("high.csv")
```

#Create Transformation Functions
```{r}
#to transform and clean API
APITransform <- function(dataset){
  if(is.data.frame(dataset)){
  dataset <- transform(dataset, 
                  API = mapvalues(API, c("Low","High","?"), 
                                     c(0, 1, NA)))
   #Change ? to NA
   dataset[dataset == "?"] <- NA
      
  #This removes rows where API has null values
  dataset <- dataset[-which(is.na(dataset$API)), ]
  return(dataset)
  }
}
  

#Categorical Changes
 ChangeCategorical <-function(dataset){
     if(is.data.frame(dataset)){
        
       #Change to numeric 
        dataset$ACS_CORE <- as.numeric(dataset$ACS_CORE)
      
       #Find the median(for ACS_CORE)
        median.ASC<- median(dataset$ACS_CORE, na.rm = TRUE)
        max.ASC <- max(dataset$ACS_CORE,na.rm = TRUE)
        dataset$ACS_CORE <- cut(dataset$ACS_CORE, c(1, median.ASC + 1, max.ASC),labels
                                     = FALSE)
        dataset$ACS_CORE[is.na(dataset$ACS_CORE)] <- 3
      
       #change ACS_CORE to categorical
        dataset$ACS_CORE <- factor(dataset$ACS_CORE)
      
       # Change Charter
        dataset$CHARTER[is.na(dataset$CHARTER)] <- 0
        dataset$CHARTER[dataset$CHARTER == "Y"] <- 1
        dataset$CHARTER[dataset$CHARTER == "D"] <- 2
        #change Year Round
        dataset$YR_RND[is.na(dataset$YR_RND)] <- 2
        dataset$YR_RND[dataset$YR_RND == "Yes"] <- 0
        dataset$YR_RND[dataset$YR_RND == "No"] <- 1

        #change to categorical
        dataset$CHARTER <- factor(dataset$CHARTER)
        dataset$YR_RND <- factor(dataset$YR_RND)
          
        #Change null values to 6
        dataset$AVG_ED[is.na(dataset$AVG_ED)] <- 6
        dataset$AVG_ED <- as.numeric(dataset$AVG_ED)
        dataset$AVG_ED <- round(dataset$AVG_ED, digits = 0)

        dataset$AVG_ED <- as.factor(dataset$AVG_ED)
        
        #Converting the Columns 
        dataset$API <- factor(dataset$API)
        dataset$VALID <- as.numeric(dataset$VALID)
        dataset$NUMTEACH <- as.numeric(dataset$NUMTEACH)
        dataset$FULL_PCT <- as.numeric(dataset$FULL_PCT)
        dataset$EMER_PCT <- as.numeric(dataset$EMER_PCT)
        dataset$WVR_PCT <- as.numeric(dataset$WVR_PCT)
        dataset$YRS_TEACH <- as.numeric(dataset$YRS_TEACH)
        dataset$YRONE_TCH <- as.numeric(dataset$YRONE_TCH)
        dataset$YRTWO_TCH <- as.numeric(dataset$YRTWO_TCH)
       
       return(dataset)
     }
 }

```

#Transform Data
```{r}
elementary.reg = APITransform(elementary)
elementary.reg = ChangeCategorical(elementary.reg)

high.reg = APITransform(high)
high.reg = ChangeCategorical(high.reg)

middle.reg = APITransform(middle)
middle.reg = ChangeCategorical(middle.reg)
```


#Logistic regression-Elementary School 

```{r}
#Logistic regression 
elementary.log <- glm(API ~., family = binomial, data = elementary.reg) 
elemLog.summary <- summary(elementary.log)
kable(elemLog.summary$coefficients)

ggplot(elementary.reg, aes(elementary.log$fitted.values, fill = as.factor(elementary.log$y))) + geom_histogram() + labs(fill = "API", title = "Estimated API Probabilities Histogram", x = "Estimated API Probabilities") + theme_classic()

```

#Logistic Regression - Middle School
```{r}
middle.log <- glm(API ~., family = binomial, data = middle.reg) 
kable(summary(middle.log)$coefficients )
ggplot(middle.reg, aes(middle.log$fitted.values, fill = as.factor(middle.log$y))) + geom_histogram() + labs(fill = "API", title = "Estimated API Probabilities Histogram", x = "Estimated API Probabilities") + theme_classic()
```


```{r}
high.log <- glm(API ~., family = binomial, data = high.reg) 
kable(summary(high.log)$coefficients)
ggplot(high.reg, aes(high.log$fitted.values, fill = as.factor(high.log$y))) + geom_histogram() + labs(fill = "API", title = "Estimated API Probabilities Histogram", x = "Estimated API Probabilities") + theme_classic()
```

#lasso for Elementary
```{r}
#removing all rows where API is null
elementary.reg2 = elementary.reg

elem.x <- model.matrix(API~.,elementary.reg2)[,-1]
elem.y <- as.matrix(data.frame(elementary.reg$API))
#Need to change y to numeric to run cross validation
elem.y <- as.numeric(elem.y)

full <- data.frame(elem.x,elem.y)
# Split data into test and train
set.seed(1)
elem.train <- sample(1:nrow(elem.x), nrow(elem.x)/2)
e<-as.data.frame(elem.train)
elem.test <- (-elem.train)
elem.y.test <- elem.y[elem.test]

# If we want to specify the number of lambda values - not sure we actually need this
# Predefined grid of lambda values: 
# grid=10^seq(10,-2, length =100)

#lasso
elem.glm <- glmnet(elem.x[elem.train,],elem.y[elem.train],family = "binomial",alpha=1) 
elem.glm
#Plot the lasso
plot(elem.glm)

#Fit the model on our test data
set.seed(1)
elem.out=cv.glmnet(elem.x[elem.train,],elem.y[elem.train],alpha=1)
plot(elem.out)
```

#Determining coeffcients and significant predictors using lasso - Elementary
```{r,warning=FALSE}
#Figuring out the minimum lambda value and predicting CV error on test
elem.bestlam=elem.out$lambda.min
elem.1se=elem.out$lambda.1se
elem.lasso.pred=predict(elem.glm,s=elem.bestlam,newx=elem.x[elem.test,])
print("Minimum lambda for Elementary")
elem.bestlam
print("1SE lambda for Elementary")
elem.1se
print("CV Error for Elementary")
mean((elem.lasso.pred-elem.y.test)^2)

#Determine which variables are non-zero at the 1se value of lambda
out=glmnet(elem.x,elem.y,alpha=1,family="binomial")
elem.lasso.coef=predict(out,type="coefficients",s=elem.1se)
elem.lasso.coef
elem.lasso.coef[elem.lasso.coef!=0]
```


## forward stepwise for elementary
```{r}
forwardStepwise <- function(dataset,title){
  if(is.data.frame(dataset)){
    forward <- regsubsets(API~.,
                                    data=dataset, 
                                    nbest = 1, 
                                    nvmax = NULL, 
                                    method="forward", really.big=TRUE)
    # display variables in each model for the fist 12 models
    for(i in 1:12){
      co = coef(forward, id=i)
      print(i)
      print(co)
    }
    
    summary = summary(forward)
  
    # plot rsq on the y-axis
    plot(summary$rsq, xlab="model size", ylab="R-squared", type = "l",main = paste(title,"R-Squared", sep="\n"))
    # plot showing BIC on the y-axis and
    plot(summary$bic, xlab="Model Size", ylab="BIC",type = "l", main = paste(title,"BIC",sep="\n"))
    return(summary)
  }
}
```

```{r}

elem.stepWise <- forwardStepwise(elementary.reg, "Elementary School")
middle.stepWise <- forwardStepwise(middle.reg,"Middle School")
high.stepWise <- forwardStepwise(high.reg, "High School")
```

#Lasso for Middle School
```{r}
#removing all rows where API is null
middle.reg2 = middle.reg

middle.x <- model.matrix(API~.,middle.reg2)[,-1]
middle.y <- as.matrix(data.frame(middle.reg$API))
middle.y <- as.numeric(middle.y)

# Split data into test and train
set.seed(1)
middle.train <- sample(1:nrow(middle.x), nrow(middle.x)/2)
middle.test <- (-middle.train)
middle.y.test <- middle.y[middle.test]

# If we want to specify the number of lambda values - not sure we actually need this
# Predefined grid of lambda values: 
# grid=10^seq(10,-2, length =100)

#lasso
middle.glm <- glmnet(middle.x[middle.train,],middle.y[middle.train],family = "binomial",alpha=1) 
middle.glm
#Plot the lasso
plot(middle.glm)

set.seed(1)
middle.out=cv.glmnet(middle.x[middle.train,],middle.y[middle.train],alpha=1)
plot(middle.out)
```

#Determining coeffcients and significant predictors using lasso - Middle
```{r,warning= FALSE}
#Figuring out the minimum lambda value and predicting CV error on test
middle.bestlam=middle.out$lambda.min
middle.1se=middle.out$lambda.1se
middle.lasso.pred=predict(middle.glm,s=middle.bestlam,newx=middle.x[middle.test,])
print("Minimum lambda for Middle")
middle.bestlam
print("1SE lambda for Middle")
middle.1se
print("CV Error for Middle")
mean((middle.lasso.pred-middle.y.test)^2)

#Determine which variables are non-zero at the 1se value of lambda
out=glmnet(middle.x,middle.y,alpha=1,family="binomial")
middle.lasso.coef=predict(out,type="coefficients",s=middle.1se)
middle.lasso.coef
middle.lasso.coef[middle.lasso.coef!=0]
```


#High School
```{r}
#removing all rows where API is null
high.reg2 = high.reg

high.x <- model.matrix(API~.,high.reg2)[,-1]
high.y <- as.matrix(data.frame(high.reg$API))
high.y <- as.numeric(high.y)

# Split data into test and train
set.seed(1)
high.train <- sample(1:nrow(high.x), nrow(high.x)/2)
high.test <- (-high.train)
high.y.test <- high.y[high.test]

# If we want to specify the number of lambda values - not sure we actually need this
# Predefined grid of lambda values: 
# grid=10^seq(10,-2, length =100)

#lasso
high.glm <- glmnet(high.x[high.train,],high.y[high.train],family = "binomial",alpha=1) 
high.glm
#Plot the lasso
plot(high.glm)

set.seed(1)
high.out=cv.glmnet(high.x[high.train,],high.y[high.train],alpha=1)
plot(high.out)
```

```{r}
#Figuring out the minimum lambda value and predicting CV error on test
high.bestlam=high.out$lambda.min
high.1se=high.out$lambda.1se
high.lasso.pred=predict(high.glm,s=high.bestlam,newx=high.x[high.test,])
print("Minimum lambda for High")
high.bestlam
print("1SE lambda for High")
high.1se
print("CV Error for High")
mean((high.lasso.pred-high.y.test)^2)

#Determine which variables are non-zero at the 1se value of lambda
out=glmnet(high.x,high.y,alpha=1,family="binomial")
high.lasso.coef=predict(out,type="coefficients",s=high.1se)
high.lasso.coef
high.lasso.coef[high.lasso.coef!=0]
```

#Classification with Naive Bayes
```{r, warning=FALSE}
elem.nb = NaiveBayes(elementary.reg$API ~ ., data = elementary.reg, usekernal = TRUE)
elem.nb.pred = predict(elem.nb,elementary.reg)
table(elem.nb.pred$class, elementary.reg$API)
cat("\n")
print("Misclassification Rate")
mean(elem.nb.pred$class != elementary.reg$API)

```

#Create Test and Train Data
```{r}
#Elementary
set.seed(1)
train1 <- sample(1:nrow(elementary.reg), nrow(elementary.reg)/2)
elementary.train <- as.data.frame(elementary.reg[train1,])

test1 <- (-train1)
elementary.test <- elementary.reg[test1,]

#Middle
set.seed(1)
train2 <- sample(1:nrow(middle.reg), nrow(middle.reg)/2)
middle.train <- as.data.frame(middle.reg[train2,])

test2 <- (-train2)
middle.test <- elementary.reg[test2,]

#High School
set.seed(1)
train3 <- sample(1:nrow(high.reg), nrow(high.reg)/2)
high.train <- as.data.frame(high.reg[train3,])

test3 <- (-train3)
high.test <- high.reg[test3,]

```
#Tree 
```{r}
treeFun <- function(dataset){
  tree <- rpart(API~.,data = dataset, method = "class")
  tree.party <- as.party(tree)
  plot(tree.party, gp = gpar(fontsize = 7))
  print(tree.party)
}
```

```{r}
treeFun(elementary.train)
treeFun(middle.train)
treeFun(high.train)
```


```

#To do......
#Convert year round to categorical-DONE LS
#Round Education and turn to categorical (current null = 6)
#plot(elementary.log)
#Check conditional probabilities with 
#Classifiers
#Logistic Lasso (Which ones are selected that are different than zero): Convert model to matrix model. Model.matrix. 
#Forward Selection.
#Clustering


