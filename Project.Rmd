---
title: "Data Mining Project"
author: "Nadine Fares, Leyang Shi, Bai Xue"
date: "12/8/2017"
output: 
  html_document:
    toc: true
    toc_depth: 4
    theme: lumen
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries
```{r, message = FALSE}
library(ggplot2)
library(plyr)
library(ISLR)
library(MASS)
library(knitr)
library(gam)
library(readr)
library(glmnet)
library(rmarkdown)
library(rpart)
library(klaR) 
library(leaps) 
library(partykit)
library(randomForest)
library(dplyr)
library(pROC)
```

#Introduction
We have three datasets (Elementary, Middle, and High) that consist of various performance parameters for California elementary, middle and high schools, collected by California Department of Education. Our measure of performance is API, which is derived from the results of the Standardized Testing and Reporting Program (STAR) and the California High School Exit Examination (CAHSEE). Our data has 17 composite features spanning 8,874 schools.In the data provided, API scores are binarized: the top 50-percentile of schools in each of the three groups is labeled as “High”, the rest as “Low” API institutions 

>Problems we are addressing

1. What are the Key Determinents of School Success? 
2. Do key determinants and their importance vary between school levels? 
3. Are ethnic factors significant predictors of school performance? Is their importance consistent across school levels?
4. Could discrepancies in performance be reduced by actively reallocating teachers’ body?


#Section 1
## Methods
We used **predictive** methods for our analysis. 

###Overview 

1. **Cleaned/Transformed the data** :This is the first thing that we did. We first transformed API scores to 0,1. We also converted '?' in the API column to NAs since we couldn't find a logical meaning for it. We then dropped all the rows where the API column had NULL values. We also had a lot of issues with NULL values, so, as instructed by Dr. C, we changed NULL values to categorical variables. We also changed other variables to categorical as well. The ones we changed were

      a. ACS_CORE: We changed this to a categorical variable of 0 and 1. 0 represents that they had less core courses than the median and 1 represents that they had more core courses than the median. 3 was used to represent NAs.
      
      b. CHARTER: We changed ?s to 0 which represents that they are not-charter schools. 1 represents that they are not funded. 2 represents that they are funded. 
      
      c. YR_RND: 0 = "Yes", 1 = "No", 2 = 'NA'
      
      d. AVG_ED: For this one, we made it categorical as well. We rounded the average education and then turned it into a factor. 6 = 'NA'.
      
      e. We then converted the others to Numeric and Factors.

    
2. **Split test and training data** : We then split our training and test data to be used later in our models. We did this for all three data sets.   

3. **Included Class Metrics Function** : We included the class metrics function from Homework 5 so that we could use it later. 

4. **Fit Logistic Regressions** : We started with the simplist model first that we thought was appropriate. We did this on all three datasets. We did two thing with this:

      a. Fit full logistic regressions: We did this on all datasets and observed the variables that were statistically significant. We found some similarities among the models. For example: in all three school levels, VALID, AA_SIGYes, AS_SIGYes, WH_SigYes, and NUMTEACH were statistically significant. There were some variations across the school levels in our logistic regressions. Elementary has additional variables that were statistically significant: CHARTER2,SD_SIGYes,YR_RND2,ACS_CORE3, AVG_ED3, AVG_ED4, AVG_ED5,AVG_ED6,and YRTWO_TEACH. Middle has additional variables that were statistically significant: CHARTER2,FULL_PCT, and YRONETCH. High also has addition variables that were statistically significant: SD_SIGYes, AVG_ED3, AVG_ED4,AVG_ED6.
      
      b. Confusion Matrix: For each logistic regression, we then constructed confusion matrixes for both the training data and test data for each dataset. We also got the missclassification rate for each. Our results were pretty good. The misclassification rates ranges from 0.13 to 0.19 for the school levels. The misclassification rates for the test confusion matrixes were relatively close to those for the training confusion matrixes. Although, these aren't perfect, it is still relatively good.! 
      
5. **Fit Forward Stepwise** : We then fit forward stepwise on all the school level data. Then, we selected the models for each with the lowest BIC. For elementary and high school, the forward stepwise selected models of 14 variables. The model for middle school selected 15. 

6. **Fit Lasso** : The issue with the forward stepwise is that it has severe problems in the presence of collinearity. It also gives biased coefficients that need shrinkage. For this reason, we chose to fit Lassos on our three datasets. We did this by giguring out the minimum lambda value and predicting CV error on test. Then we determined which variables are non-zero at the 1se value of lambda. This resulted in smaller models for Middle and High datasets.

7. **Naive Bayes** : We did not end up using this model, but we did it just to see how it would work. The misclassification rates ranged in 0.20s. This isn't the worst, but we found no reason to include it in the model since it didn't help us answer our project questions. 

8. **Decision Trees** : We did these things: 

      a. Fit unpruned decision trees on all three training datasets: This came out with some interesting models. For elementary: The variables that the tree split on were AVG_ED (twice), WH_SIG, and AS_SIG. For middle: The variables that the tree split on were AVG_ED(trice), FULL_PCT, YRS_TEACH, AS_SIG, WH_SIG, and VALID (twice). For high; The variables that the tree split on were AVG_ED, Valid, and AVG_ED again.
      b. Fit pruned decision trees on all three training datasets: We did this based on the 1SE rule. This really narrowed things down. For elementary: the only variable tree split on was AVG_ED. The same occured for middle. For High: the only variables the tree split on was AVG_ED and VALID.
      c. Confusion Matrix on test data: For all the models the accuracy was about 80%, which isn't bad.
      d. Notice fault in decision trees. This didn't help us that much with picking the variables. The pruned trees ended up being a little meaningless especially with NA as a categorical variable for AVG_ED (represented by 6)

9. **Random Forest** : We then ran random forests on the training data and then tested it on our test data. We came out with about 80% accuracy. We also had nice variable importance plots that we found this very useful. 

10. **Compare to Prevalence** :Last thing we did was get the prevelance of each data set so we could make sure we were doing okay comparatively.

>Models Used

1.Lasso

2.Random Forest

###Cleaning and transforming the Data
##### Read in data and read convert values to 0/1/NA
```{r, warning=FALSE, message = FALSE}
#linear regression
elementary <- read_csv("elementary.csv")

middle <- read_csv("middle.csv")

high <- read_csv("high.csv")
```

#####Create Transformation Functions
```{r}
#to transform and clean API
APITransform <- function(dataset){
  if(is.data.frame(dataset)){
  dataset <- transform(dataset, 
                  API = mapvalues(API, c("Low","High","?"), 
                                     c(0, 1, NA)))
   #Change ? to NA
   dataset[dataset == "?"] <- NA
      
  #This removes rows where API has null values
  dataset <- dataset[-which(is.na(dataset$API)), ]
  return(dataset)
  }
}

#Categorical Changes
 ChangeCategorical <-function(dataset){
     if(is.data.frame(dataset)){
        
       #Change to numeric 
        dataset$ACS_CORE <- as.numeric(dataset$ACS_CORE)
      
       #Find the median(for ACS_CORE)
        median.ASC<- median(dataset$ACS_CORE, na.rm = TRUE)
        max.ASC <- max(dataset$ACS_CORE,na.rm = TRUE)
        dataset$ACS_CORE <- cut(dataset$ACS_CORE, c(1, median.ASC + 1, max.ASC),labels
                                     = FALSE)
        dataset$ACS_CORE[is.na(dataset$ACS_CORE)] <- 3
      
       #change ACS_CORE to categorical
        dataset$ACS_CORE <- factor(dataset$ACS_CORE)
      
       # Change Charter
        dataset$CHARTER[is.na(dataset$CHARTER)] <- 0
        dataset$CHARTER[dataset$CHARTER == "Y"] <- 1
        dataset$CHARTER[dataset$CHARTER == "D"] <- 2
        #change Year Round
        dataset$YR_RND[is.na(dataset$YR_RND)] <- 2
        dataset$YR_RND[dataset$YR_RND == "Yes"] <- 0
        dataset$YR_RND[dataset$YR_RND == "No"] <- 1

        #change to categorical
        dataset$CHARTER <- factor(dataset$CHARTER)
        dataset$YR_RND <- factor(dataset$YR_RND)
          
        #Change null values to 6
        dataset$AVG_ED[is.na(dataset$AVG_ED)] <- 6
        dataset$AVG_ED <- as.numeric(dataset$AVG_ED)
        dataset$AVG_ED <- round(dataset$AVG_ED, digits = 0)

        dataset$AVG_ED <- as.factor(dataset$AVG_ED)
        
        #Converting the Columns 
        dataset$API <- factor(dataset$API)
        dataset$VALID <- as.numeric(dataset$VALID)
        dataset$NUMTEACH <- as.numeric(dataset$NUMTEACH)
        dataset$FULL_PCT <- as.numeric(dataset$FULL_PCT)
        dataset$EMER_PCT <- as.numeric(dataset$EMER_PCT)
        dataset$WVR_PCT <- as.numeric(dataset$WVR_PCT)
        dataset$YRS_TEACH <- as.numeric(dataset$YRS_TEACH)
        dataset$YRONE_TCH <- as.numeric(dataset$YRONE_TCH)
        dataset$YRTWO_TCH <- as.numeric(dataset$YRTWO_TCH)
       
       return(dataset)
     }
 }

```

#####Transform Data
```{r}
elementary.reg = APITransform(elementary)
elementary.reg = ChangeCategorical(elementary.reg)

high.reg = APITransform(high)
high.reg = ChangeCategorical(high.reg)

middle.reg = APITransform(middle)
middle.reg = ChangeCategorical(middle.reg)
```

#####Test and Training Data
```{r}
#Elementary
set.seed(1)
train1 <- sample(1:nrow(elementary.reg), nrow(elementary.reg)/2)
elementary.train <- as.data.frame(elementary.reg[train1,])

test1 <- (-train1)
elementary.test <- elementary.reg[test1,]

#Middle
set.seed(1)
train2 <- sample(1:nrow(middle.reg), nrow(middle.reg)/2)
middleschool.train <- as.data.frame(middle.reg[train2,])

test2 <- (-train2)
middleschool.test <- middle.reg[test2,]

#High School
set.seed(1)
train3 <- sample(1:nrow(high.reg), nrow(high.reg)/2)
highschool.train <- as.data.frame(high.reg[train3,])

test3 <- (-train3)
highschool.test <- high.reg[test3,]

```

#####Class Metrics Function 
**Note: This metrics funtion is from homework 5**
```{r}
classMetrics <- function(score, y, cutoff, 
                         type = c("all", "accuracy", "sensitivity", 
                                  "specificity", "ppv", "npv", "precision", 
                                  "recall")) {
  type <- match.arg(type, several.ok = TRUE)
  n <- length(y) 
  
  # Form confusion matrix
  score.factor <- factor(as.numeric(score >= cutoff), levels = c("0", "1"))
  confusion.mat <- table(score.factor, as.factor(y), dnn = list("predicted", "observed"))

  # Calculate all metrics
  acc <- sum(diag(confusion.mat)) / n
  sens <- confusion.mat[2,2] / sum(confusion.mat[,2])
  spec <- confusion.mat[1,1] / sum(confusion.mat[,1])
  ppv <- confusion.mat[2,2] / sum(confusion.mat[2,])
  npv <- confusion.mat[1,1] / sum(confusion.mat[1,])
  prec <- ppv
  rec <- sens
  
  metric.names <- c("accuracy", "sensitivity", "specificity", 
                    "ppv", "npv", "precision", "recall")
  metric.vals <- c(acc, sens, spec, ppv, npv, prec, rec)
  
  # Form into data frame
  full.df <- data.frame(value = metric.vals)
  rownames(full.df) <- metric.names
  
  # Return just the requested subset of metrics
  if(type[1] == "all") {
    list(conf.mat = confusion.mat, perf = full.df)
  } else {
    list(conf.mat = confusion.mat, 
         perf = subset(full.df, subset = metric.names %in% type))
  }
}

plotClassMetrics <- function(score, y, xvar = NULL, yvar = c("accuracy", "sensitivity", 
                                  "specificity", "ppv", "npv", "precision", 
                                  "recall"),
                             flip.x = FALSE) {
  yvar <- match.arg(yvar)
  
  # If there are a lot of unique score values, just calculate the metrics
  # along an evenly spaced grid of 100 scores.
  unique.scores <- unique(score)
  if(length(unique.scores) > 100) {
    cutoff.seq <- sample(unique.scores, 100, replace = FALSE)
  } else {
    cutoff.seq <- unique.scores
  }
  
  n <- length(cutoff.seq)
  
  x.out <- numeric(n)
  y.out <- numeric(n)
  # Loop over all values of the score and calculate the performance metrics
  for(i in 1:n) {
    if(!is.null(xvar)) {
      metrics <- classMetrics(score, y, cutoff = cutoff.seq[i], type = c(xvar, yvar))
      x.out[i] <- metrics$perf[xvar, 1]
      y.out[i] <- metrics$perf[yvar, 1]
    } else {
      metrics <- classMetrics(score, y, cutoff = cutoff.seq[i], type = c(yvar))
      x.out[i] <- cutoff.seq[i]
      y.out[i] <- metrics$perf[yvar, 1]
    }
  }
  # Combine metrics into a data frame
  if(flip.x) {
    x.out <- 1 - x.out
  }
  df.out <- data.frame(score = cutoff.seq, x = x.out, y = y.out)
  # Reorder the data frame in increasing order of the x-axis variable
  df.out <- df.out[order(df.out$score), ]
  # De-duplicate x-axis
  df.out <- subset(df.out, subset = !duplicated(df.out$x))
  
  # Construct line plot
  if(!is.null(xvar)) {
    x.text <- ifelse(flip.x, paste0("1 - ", xvar), xvar)
  } else {
    x.text <- "score"
  }
  
  print(qplot(data = df.out, x = x, y = y, geom = "line",
              xlab = ifelse(is.null(xvar), "score", x.text),
              ylab = yvar, ylim = c(0, 1)))
}
```


### Models

####Logistic regressions
**In the following, we fit logistic regressions on all the grade level data separately (elementary school, middle school, and high school) and plotted API probability histograms**
##### Elementary School
```{r}
#Logistic regression 
elementary.log <- glm(API ~., family = binomial, data = elementary.reg) 

#ggplot(elementary.train, aes(elementary.log$fitted.values, fill = as.factor(elementary.log$y))) + geom_histogram() + labs(fill = "API", title = "Estimated API Probabilities Histogram", x = "Estimated API Probabilities") + theme_classic()

data.frame(summary(elementary.log)$coef[summary(elementary.log)$coef[,4] <= .05, 1])

```

**For elementary school data, the variables that are statistically significant (p < 0.05) are Charter 2(negative), Valid(positive), AA_SIGYes(negative), AS_SIGYes(positive), WH_SIGYes(positive), SD_SIGYes(negative), YR_RND2(negative),ACS_CORE3(positive),AVG_ED3(positive),AVG_ED4(positive),AVG_ED5(positive), AVG_ED6(positive), NUMTEACH(negative), and YRTWO_TCH(negative).**

###### Confusion Matrix
```{r}
cat("Training Data Confusion Matrix \n")
elementary.logit <- glm(API ~., family = binomial, data = elementary.train) 
# Training data confusion matrix
confusion.logit.train <- table(round(elementary.logit$fitted), elementary.logit$y)
confusion.logit.train
#Training data misclassification rate
1 - sum(diag(confusion.logit.train)) / sum(confusion.logit.train)

cat(" \n Test Data Confusion Matrix")
# Test data confusion matrix
test.probs <- predict(elementary.logit, elementary.test, type = "response")
confusion.logit.test <- table(round(test.probs), elementary.test$API)
confusion.logit.test

# Test data misclassification rate
1 - sum(diag(confusion.logit.test)) / sum(confusion.logit.test)

```
##### Middle School
```{r}
middle.log <- glm(API ~., family = binomial, data = middle.reg) 
#ggplot(middle.reg, aes(middle.log$fitted.values, fill = as.factor(middle.log$y))) + geom_histogram() + labs(fill = "API", title = "Estimated API Probabilities Histogram", x = "Estimated API Probabilities") + theme_classic()

data.frame(summary(middle.log)$coef[summary(middle.log)$coef[,4] <= .05, 1])

```

**Middle school data is slightly different. The variables that are statistically significant (p < 0.05) are Charter 2(positive), Valid(positive), AA_SIGYes(negative), AS_SIGYes(positive), WH_SIGYes(positive), NUMTEACH(negative), and FULL_PCT(positive). **

###### Confusion Matrix
```{r}
cat("Training Data Confusion Matrix \n")
middle.logit <- glm(API ~., family = binomial, data = middleschool.train) 
# Training data confusion matrix
confusion.logit.train <- table(round(middle.logit$fitted), middle.logit$y)
confusion.logit.train
#Training data misclassification rate
1 - sum(diag(confusion.logit.train)) / sum(confusion.logit.train)

cat(" \n Test Data Confusion Matrix")
# Test data confusion matrix
test.probs <- predict(middle.logit, middleschool.test, type = "response")
confusion.logit.test <- table(round(test.probs), middleschool.test$API)
confusion.logit.test

# Test data misclassification rate
1 - sum(diag(confusion.logit.test)) / sum(confusion.logit.test)
```
##### High School
```{r}
high.log <- glm(API ~., family = binomial, data = high.reg) 
#ggplot(high.reg, aes(high.log$fitted.values, fill = as.factor(high.log$y))) + geom_histogram() + labs(fill = "API", title = "Estimated API Probabilities Histogram", x = "Estimated API Probabilities") + theme_classic()

data.frame(summary(high.log)$coef[summary(high.log)$coef[,4] <= .05, 1])

```

**High School Data also varies from the others.The variables that are statistically significant (p < 0.05) are Valid (positive), AA_SIGYes(negative), AS_SIGYes(positive), WH_SIGYes(positive), SD_SIGYes(positive),AVG_ED4(positive),AVG_ED6(positive), and NUMTEACH(negative).**
######Confusion Matrix
```{r}
cat("Training Data Confusion Matrix \n")
high.logit <- glm(API ~., family = binomial, data = highschool.train) 
# Training data confusion matrix
confusion.logit.train <- table(round(high.logit$fitted), high.logit$y)
confusion.logit.train
#Training data misclassification rate
1 - sum(diag(confusion.logit.train)) / sum(confusion.logit.train)

cat(" \n Test Data Confusion Matrix")
# Test data confusion matrix
test.probs <- predict(high.logit, highschool.test, type = "response")
confusion.logit.test <- table(round(test.probs), highschool.test$API)
confusion.logit.test

# Test data misclassification rate
1 - sum(diag(confusion.logit.test)) / sum(confusion.logit.test)
```

#### Forward Stepwise

##### Function
```{r}
forwardStepwise <- function(dataset,title){
  if(is.data.frame(dataset)){
    forward <- regsubsets(API~.,
                                    data=dataset, 
                                    nbest = 1, 
                                    nvmax = NULL, 
                                    method="forward", really.big=TRUE)
    # display variables in each model for the fist 12 models
    for(i in 1:12){
      co = coef(forward, id=i)
      print(i)
      print(co)
    }
    
    summary = summary(forward)
  
    # plot rsq on the y-axis
    plot(summary$rsq, xlab="model size", ylab="R-squared", type = "l",main = paste(title,"R-Squared", sep="\n"))
    # plot showing BIC on the y-axis and
    plot(summary$bic, xlab="Model Size", ylab="BIC",type = "l", main = paste(title,"BIC",sep="\n"))
    return(forward)
  }
}
```
##### Elementary School
```{r}
elem.stepWise <- forwardStepwise(elementary.reg, "Elementary School")
```
###### Model with Min BIC
```{r}
min.bic <- which.min(summary(elem.stepWise)$bic)
coef(elem.stepWise, min.bic)
```
##### Middle School
```{r}
middle.stepWise <- forwardStepwise(middle.reg,"Middle School")
```

###### Model with Min BIC
```{r}
min.bic <- which.min(summary(middle.stepWise)$bic)
coef(middle.stepWise, min.bic)
```
##### High School
```{r}
high.stepWise <- forwardStepwise(high.reg, "High School")
```
######Model with Min BIC
```{r}
min.bic <- which.min(summary(high.stepWise)$bic)
coef(high.stepWise, min.bic)
```

#### Lasso
#####Elementary
```{r}
#removing all rows where API is null
elementary.reg2 = elementary.reg

elem.x <- model.matrix(API~.,elementary.reg2)[,-1]
elem.y <- as.matrix(data.frame(elementary.reg$API))
#Need to change y to numeric to run cross validation
elem.y <- as.numeric(elem.y)

full <- data.frame(elem.x,elem.y)
# Split data into test and train
set.seed(1)
elem.train <- sample(1:nrow(elem.x), nrow(elem.x)/2)
e<-as.data.frame(elem.train)
elem.test <- (-elem.train)
elem.y.test <- elem.y[elem.test]

# If we want to specify the number of lambda values - not sure we actually need this
# Predefined grid of lambda values: 
# grid=10^seq(10,-2, length =100)

#lasso
elem.glm <- glmnet(elem.x[elem.train,],elem.y[elem.train],family = "binomial",alpha=1) 
elem.glm
#Plot the lasso
plot(elem.glm)

#Fit the model on our test data
set.seed(1)
elem.out=cv.glmnet(elem.x[elem.train,],elem.y[elem.train],alpha=1)
plot(elem.out)
```

######Determining coeffcients and significant predictors using lasso - Elementary
```{r,warning=FALSE}
#Figuring out the minimum lambda value and predicting CV error on test
elem.bestlam=elem.out$lambda.min
elem.1se=elem.out$lambda.1se
elem.lasso.pred=predict(elem.glm,s=elem.bestlam,newx=elem.x[elem.test,])
print("Minimum lambda for Elementary")
elem.bestlam
print("1SE lambda for Elementary")
elem.1se
print("CV Error for Elementary")
mean((elem.lasso.pred-elem.y.test)^2)

#Determine which variables are non-zero at the 1se value of lambda
out=glmnet(elem.x,elem.y,alpha=1,family="binomial")
elem.lasso.coef=predict(out,type="coefficients",s=elem.1se)
elem.lasso.coef
elem.lasso.coef[elem.lasso.coef!=0]
```


#####Middle School
```{r}
#removing all rows where API is null
middle.reg2 = middle.reg

middle.x <- model.matrix(API~.,middle.reg2)[,-1]
middle.y <- as.matrix(data.frame(middle.reg$API))
middle.y <- as.numeric(middle.y)

# Split data into test and train
set.seed(1)
middle.train <- sample(1:nrow(middle.x), nrow(middle.x)/2)
middle.test <- (-middle.train)
middle.y.test <- middle.y[middle.test]

# If we want to specify the number of lambda values - not sure we actually need this
# Predefined grid of lambda values: 
# grid=10^seq(10,-2, length =100)

#lasso
middle.glm <- glmnet(middle.x[middle.train,],middle.y[middle.train],family = "binomial",alpha=1) 
middle.glm
#Plot the lasso
plot(middle.glm)

set.seed(1)
middle.out=cv.glmnet(middle.x[middle.train,],middle.y[middle.train],alpha=1)
plot(middle.out)
```

######Determining coeffcients and significant predictors using lasso - Middle
```{r,warning= FALSE}
#Figuring out the minimum lambda value and predicting CV error on test
middle.bestlam=middle.out$lambda.min
middle.1se=middle.out$lambda.1se
middle.lasso.pred=predict(middle.glm,s=middle.bestlam,newx=middle.x[middle.test,])
print("Minimum lambda for Middle")
middle.bestlam
print("1SE lambda for Middle")
middle.1se
print("CV Error for Middle")
mean((middle.lasso.pred-middle.y.test)^2)

#Determine which variables are non-zero at the 1se value of lambda
out=glmnet(middle.x,middle.y,alpha=1,family="binomial")
middle.lasso.coef=predict(out,type="coefficients",s=middle.1se)
middle.lasso.coef
middle.lasso.coef[middle.lasso.coef!=0]
```


#####High School
```{r}
#removing all rows where API is null
high.reg2 = high.reg

high.x <- model.matrix(API~.,high.reg2)[,-1]
high.y <- as.matrix(data.frame(high.reg$API))
high.y <- as.numeric(high.y)

# Split data into test and train
set.seed(1)
high.train <- sample(1:nrow(high.x), nrow(high.x)/2)
high.test <- (-high.train)
high.y.test <- high.y[high.test]

# If we want to specify the number of lambda values - not sure we actually need this
# Predefined grid of lambda values: 
# grid=10^seq(10,-2, length =100)

#lasso
high.glm <- glmnet(high.x[high.train,],high.y[high.train],family = "binomial",alpha=1) 
high.glm
#Plot the lasso
plot(high.glm)

set.seed(1)
high.out=cv.glmnet(high.x[high.train,],high.y[high.train],alpha=1)
plot(high.out)
```
######Determining coeffcients and significant predictors using lasso - High
```{r}
#Figuring out the minimum lambda value and predicting CV error on test
high.bestlam=high.out$lambda.min
high.1se=high.out$lambda.1se
high.lasso.pred=predict(high.glm,s=high.bestlam,newx=high.x[high.test,])
print("Minimum lambda for High")
high.bestlam
print("1SE lambda for High")
high.1se
print("CV Error for High")
mean((high.lasso.pred-high.y.test)^2)

#Determine which variables are non-zero at the 1se value of lambda
out=glmnet(high.x,high.y,alpha=1,family="binomial")
high.lasso.coef=predict(out,type="coefficients",s=high.1se)
high.lasso.coef
high.lasso.coef[high.lasso.coef!=0]
```

####Classification with Naive Bayes

#####Elementary School
```{r, warning=FALSE}
elem.nb = NaiveBayes(elementary.reg$API ~ ., data = elementary.reg, usekernal = TRUE)
elem.nb.pred = predict(elem.nb,elementary.reg)
table(elem.nb.pred$class, elementary.reg$API)
cat("\n")
print("Misclassification Rate")
mean(elem.nb.pred$class != elementary.reg$API)

```
##### Middle School
```{r, warning=FALSE}
middle.nb = NaiveBayes(middle.reg$API ~ ., data = middle.reg, usekernal = TRUE)
middle.nb.pred = predict(middle.nb,middle.reg)
table(middle.nb.pred$class, middle.reg$API)
cat("\n")
print("Misclassification Rate")
mean(middle.nb.pred$class != middle.reg$API)

```
##### High School
```{r, warning=FALSE}
high.nb = NaiveBayes(high.reg$API ~ ., data = high.reg, usekernal = TRUE)
high.nb.pred = predict(high.nb,high.reg)
table(high.nb.pred$class, high.reg$API)
cat("\n")
print("Misclassification Rate")
mean(high.nb.pred$class != high.reg$API)

```

#### Decisions Trees
```{r}
treeFun <- function(dataset){
  tree <- rpart(API~.,data = dataset, method = "class")
  tree.party <- as.party(tree)
  plot(tree.party, gp = gpar(fontsize = 7))
  print(tree.party)
  return(tree)
}

treePruned <- function(tree){
   min.cv.idx <- which.min(tree$cptable[,"xerror"])
  # min CV error + 1se (this is the height of the horizontal bar)
  min.cv.err.1se <- tree$cptable[min.cv.idx,"xerror"] +
                   tree$cptable[min.cv.idx,"xstd"]
  # Which cp values produce models whose error is below min CV + 1se?
  cp.vals <- tree$cptable[which(tree$cptable[,"xerror"] < min.cv.err.1se),"CP"]

  # 1-SE rule value of cp
  cp.1se <- max(cp.vals)

  #Prune tree
  pruned.tree <- prune(tree, cp = cp.1se)
  tree.party<-as.party(pruned.tree)
  plot(tree.party, gp = gpar(fontsize = 7))
  print(tree.party)
  return(pruned.tree)
}
```
##### Elementary School Tree
```{r}
#To prune or not to prune?
elementary.tree <- treeFun(elementary.train)
elementary.tree <- treePruned(elementary.tree)
```
**Just modeling the unpruned tree uses 3 distinct predictors. Pruning the tree based on 1SE rule narrows it down to one (Avg_Ed) **
######Elementary Confusion Matrix
```{r}
elem.test.prob <- predict(elementary.tree, newdata = elementary.test, type = "prob")[,"1"]
roc.elem <- roc(elementary.test$API, elem.test.prob)   
plot(roc.elem)
classMetrics(elem.test.prob, elementary.test$API, cutoff = 0.5)

```
##### Middle School Tree
```{r}
middle.tree <- treeFun(middleschool.train)
middle.tree <- treePruned(middle.tree)
```
###### Middle School Confusion Matrix
```{r}
middle.test.prob <- predict(middle.tree, newdata = middleschool.test, type = "prob")[,"1"]
roc.middle <- roc(middleschool.test$API, middle.test.prob)   
plot(roc.middle)
classMetrics(middle.test.prob, middleschool.test$API, cutoff = 0.4)
```
##### High School Tree
```{r}
high.tree <- treeFun(highschool.train)
high.tree <- treePruned(high.tree)

```
###### High School Confusion Matrix
```{r}
high.test.prob <- predict(high.tree, newdata = highschool.test, type = "prob")[,"1"]
roc.high <- roc(highschool.test$API, high.test.prob)   
plot(roc.high)
classMetrics(high.test.prob, highschool.test$API, cutoff = 0.4)
```

#### Random Forest
#####Elementary School
```{r}
elementary.train=elementary.train %>% mutate_if(is.character, as.factor)
elementary.test=elementary.test %>% mutate_if(is.character, as.factor)
elementary.rf <- randomForest(API~., data = elementary.train, mtry=4, importance=TRUE)
print(elementary.rf)
importance(elementary.rf)
varImpPlot(elementary.rf)
predict.elem <- predict(elementary.rf, newdata = elementary.test, type = "prob")[,"1"]
classMetrics(predict.elem, elementary.test$API, cutoff = 0.3)


```


#####Middle School
```{r}
#Test?
middleschool.train=middleschool.train %>% mutate_if(is.character, as.factor)
middleschool.test=middleschool.test %>% mutate_if(is.character, as.factor)
middle.rf <- randomForest(API~., data = middleschool.train, mtry=4, importance=TRUE)
print(middle.rf)
importance(middle.rf)
varImpPlot(middle.rf)
predict.middle <- predict(middle.rf, newdata = middleschool.test, type = "prob")[,"1"]
classMetrics(predict.middle, middleschool.test$API, cutoff = 0.3)
```

#####High School
```{r}
highschool.train=highschool.train %>% mutate_if(is.character, as.factor)
highschool.test=highschool.train %>% mutate_if(is.character, as.factor)
high.rf <- randomForest(API~., data = highschool.train, mtry=4, importance=TRUE)
print(high.rf)
importance(high.rf)
varImpPlot(high.rf)
predict.high <- predict(high.rf, newdata = highschool.test, type = "prob")[,"1"]
classMetrics(predict.high, highschool.test$API, cutoff = 0.3)
```
###### Prevalence 
```{r}
cat("Number of 1s for elementary school \n")
length(which(elementary.reg$API == "1")) / nrow(elementary.reg)

cat("Number of 1s for middle school \n")
length(which(middle.reg$API == "1")) / nrow(middle.reg)

cat("Number of 1s for high school \n")
length(which(high.reg$API == "1")) / nrow(high.reg)
```

#Section 2: Key Findings, Main Takeaways
##Question 1 - What are the Key Determinents of School Success?
#Question 1

Statistically Significant Variables
Logistic Regression
Elementary:
Charter 2
Valid
AA_SIGYes
AS_SIG_Yes
WH_SIGYes
SD_SIGYes
YR_RND2
ACS_CORE3
AVG_ED3
AVG_ED4
AVG_ED5
AVG_ED6
NUMTEACH
YRTWO_TCH

Middle:
Charter 2
Valid
AA_SIGYes
AS_SIGYes
WH_SIGYes
SD_SIGYes
YR_RND2
ACS_CORE3
AVG_ED3
AVG_ED4
AVG_ED5
AVG_ED6
NUMTEACH
YRTWO_TCH

High:
VALID				
AA_SIGYes				
AS_SIGYes			
WH_SIGYes				
SD_SIGYes			
AVG_ED3				
AVG_ED4			
AVG_ED6			
NUMTEACH

Forward Stepwise Selection:
Elementary:
Charter2
AA_SIGYes
AS_SIGYes
WH_SIGYes
SD_SIGYes
YR_RND2
ACS_CORE2
ACS_CORE3
AVG_ED2
AVG_ED3
AVG_ED4
AVG_ED5
AVG_ED6
YRTWO_TCH

Middle:
Charter2
Valid
AA_SIGYes
AS_SIGYes
HI_SIGYes
WH_SIGYes
ACS_CORE2
AVG_ED2
AVG_ED3
AVG_ED4
AVG_ED5
AVG_ED6
NUMTEACH
FULL_PCT
YRONE_TCH

High:
VALID
AA_SIGYes
AS_SIGYes
WH_SIGYes
SD_SIGYes
YR_RND1
AVG_ED2
AVG_ED3
AVG_ED4
AVG_ED5
AVG_ED6
NUMTEACH
EMER_PCT
YRONE_TCH

Lasso
Elementary:
Charter2
AA_SIGYes
AS_SIGYes
WH_SIGYes
SD_SIGYes
YR_RND2
ACS_CORE3
AVG_ED2
AVG_ED3
AVG_ED4
AVG_ED5
YRS_TEACH
YRONE_TCH
TRTWO_TCH

Middle:
Charter2
AA_SIGYes
AS_SIGYes
HI_SIGYes
WH_SIGYes
ACS_Core2
AVG_ED2
AVG_ED4
FULL_PCT
YRONE_TCH

High:
AA_SIGYes
AS_SIGYes
WH_SIGYes
SD_SIGYes
YR_RND1
AVG_ED2
AVG_ED4
EMER_PCT

Pruned Tree:
Elementary:
AVG_ED

Middle
AVG_ED

High
AVG_ED
Valid

Random Forest
Elementary: (In order of importance)
AVG_ED
WH_SIG
NUMTEACH
VALID
AS_SIG
SD_SIG

Middle: (In order of importance)
AVG_ED
WH_SIG
FULL_PCT
HI_SIG
VALID
AS_SIG

High: (In order of importance)
AVG_ED
VALID
WH_SIG
AS_SIG
FULL_PCT
NUMTEACH

**Across all of our models, AVG_ED is the most important. If you see on our Random Forests, this is the most important factor for all three school levels. ALso in our pruned decisions trees(based on minimum BIC): elementary and middle school only had AVG_ED, while highschool had AVG_ED and VALID). Additionally, in our variable selection models (Logistic Regression, Forward Stepwise Selection, and Lasso), AVG_ED, CHARTER2, VALID, WH_SIG, AS_SIG, YRTWO_TEACH, YRS_TEACH, NUMTEACH, and AS_SIG were the most common variables being selected across all school levels. There is some variation between the school levels which we will discuss in the next question. Ultimately, the model we decided to choose was our Random Forests because it managed to acheive 80% accuracy for predicting the elementary and middle school data and 97% accuracy for high school data. This is in comparison to the roughly 70% accuracy across all levels using our Naive Bayes model and our pruned trees. Additionally, Random Forests have a reputation of not overfitting, so we believe that it is the best classifier model for this dataset. Under our selected model, the most important variable factors are AVG_ED, Valid, WH_SIG, AS_SIG, NUMTEACH and FULL_PCT.**
  
## Question 2 - Do key determinants and their importance vary between school levels?
**The key determinants and their importance do vary slightly between school levels. Under our Random Forest model, most variables stay the same between school levels. AVG_ED, WH_SIG, AS_SIG, and VALID show up across all levels. NUMTEACH is significant in both high and elementary. FULL_PCT shows up in both middle and high school. When we look at our logistic regression, forward stepwise selection, and lasso models, we can see that the teacher variables (YRS_TEACH, NUMTEACH, YRTWO_TEACH, YRONE_TEACH) show up significantly more often during elementary and middle school. This seems to imply that the quality and experience of teachers are more important in early education than later education. Additionally, Charter2 (Which indicates funded-Charter schools) are more of a determining factor for elementary and middle schools. This variable is not important for high schools.**

##Question 3 - Ethnic Factors and Their Importance

###Are ethnic factors significant predictors of school performance? 
** Based off of our random forest model, ethnic factors are significant predictors of school performance.  **

###Is Their importance consistent across school levels?
**Answer Here ** 

##Question 4 - Could discrepancies in performance be reduced by actively reallocating teachers’ body?
**Answer here**

##Data Limitations
**There are some limitations with this data. First of all, We didn't like how the scores were ranked "Low" and "High". This means that there is a cutoff and we are not considering the people right at the end or beginning of the cutoff. For example: If the cutoff score is 50, we are categorizing people who made 49 differently even though they might be the same as those who made 50. Also, we are testing different tests among different school levels, so it's hard to compare across them. In addition to the issues states above, there were too many NULL values. Some of the findings we got were useless because it involved a categorical variable that represented 'NA'**

